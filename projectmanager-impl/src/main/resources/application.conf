#
#
play.application.loader = io.surfkit.projectmanager.impl.ProjectManagerLoader


######################################
# Persistence (Cassandra) Configuration
######################################

//Use same keyspace for journal, snapshot-store and read-side.
//Each service should be in a unique keyspace.
projectmanager.cassandra.keyspace = projectmanager

//For simplicity, keyspaces and tables will be auto-created on startup.
//This is often not desirable in production environments, thus these may be
//set to false to require manual creation. In that case, if the keyspace or
//tables do not exist, the service will log an error and fail to start.
cassandra-journal{
  keyspace = ${projectmanager.cassandra.keyspace}
  keyspace-autocreate = true
  tables-autocreate = true
}
cassandra-snapshot-store{
  keyspace = ${projectmanager.cassandra.keyspace}
  keyspace-autocreate = true
  tables-autocreate = true
}
lagom.persistence.read-side.cassandra{
  keyspace = ${projectmanager.cassandra.keyspace}
  keyspace-autocreate = true
}


# The properties below override Lagom default configuration with the recommended values for new projects.
#
# Lagom has not yet made these settings the defaults for backward-compatibility reasons.

# Prefer 'ddata' over 'persistence' to share cluster sharding state for new projects.
# See https://doc.akka.io/docs/akka/current/cluster-sharding.html#distributed-data-vs-persistence-mode
akka.cluster.sharding.state-store-mode = ddata

# Enable the serializer provided in Akka 2.5.8+ for akka.Done and other internal
# messages to avoid the use of Java serialization.
akka.actor.serialization-bindings {
  "akka.Done"                 = akka-misc
  "akka.actor.Address"        = akka-misc
  "akka.remote.UniqueAddress" = akka-misc
}

# https://discuss.lightbend.com/t/no-configuration-setting-found-for-key-decode-max-size/2738/3
akka.http.routing.decode-max-size = 8m

cassandra-query-journal.refresh-interval = 1s

lagom.persistence {

  # As a rule of thumb, the number of shards should be a factor ten greater
  # than the planned maximum number of cluster nodes. Less shards than number
  # of nodes will result in that some nodes will not host any shards. Too many
  # shards will result in less efficient management of the shards, e.g.
  # rebalancing overhead, and increased latency because the coordinator is
  # involved in the routing of the first message for each shard. The value
  # must be the same on all nodes in a running cluster. It can be changed
  # after stopping all nodes in the cluster.
  max-number-of-shards = 100

  # Persistent entities saves snapshots after this number of persistent
  # events. Snapshots are used to reduce recovery times.
  # It may be configured to "off" to disable snapshots.
  # Author note: snapshotting turned off
  snapshot-after = off

  # A persistent entity is passivated automatically if it does not receive
  # any messages during this timeout. Passivation is performed to reduce
  # memory consumption. Objects referenced by the entity can be garbage
  # collected after passivation. Next message will activate the entity
  # again, which will recover its state from persistent storage. Set to 0
  # to disable passivation - this should only be done when the number of
  # entities is bounded and their state, sharded across the cluster, will
  # fit in memory.
  # Author note: Set to one day - this may be a bit long for production.
  passivate-after-idle-timeout = 86400s

  # Specifies that entities run on cluster nodes with a specific role.
  # If the role is not specified (or empty) all nodes in the cluster are used.
  # The entities can still be accessed from other nodes.
  run-entities-on-role = ""

  # Default timeout for PersistentEntityRef.ask replies.
  # Author note: Made longer to support potentially slower Minikube environment
  ask-timeout = 60s

  dispatcher {
    type = Dispatcher
    executor = "thread-pool-executor"
    thread-pool-executor {
      fixed-pool-size = 16
    }
    throughput = 1
  }
}

lagom.persistence.read-side {

  # how long should we wait when retrieving the last known offset
  # LROK Author Note: Default is 5s, this has been made longer for the course.
  offset-timeout = 60s

  # Exponential backoff for failures in ReadSideProcessor
  failure-exponential-backoff {
    # minimum (initial) duration until processor is started again
    # after failure
    min = 3s

    # the exponential back-off is capped to this duration
    max = 30s

    # additional random delay is based on this factor
    random-factor = 0.2
  }

  # The amount of time that a node should wait for the global prepare callback to execute
  # LROK Author Note: Default is 20s, this has been made longer for the course.
  global-prepare-timeout = 60s

  # Specifies that the read side processors should run on cluster nodes with a specific role.
  # If the role is not specified (or empty) all nodes in the cluster are used.
  run-on-role = ""

  # The Akka dispatcher to use for read-side actors and tasks.
  use-dispatcher = "lagom.persistence.dispatcher"
}
#//#persistence-read-side

######################################
# Message Broker (Kafka) Configuration
######################################

lagom.broker.kafka {
  # The name of the Kafka service to look up out of the service locator.
  # If this is an empty string, then a service locator lookup will not be done,
  # and the brokers configuration will be used instead.
  service-name = "kafka_native"
  service-name = ${?KAFKA_SERVICE_NAME}

  # The URLs of the Kafka brokers. Separate each URL with a comma.
  # This will be ignored if the service-name configuration is non empty.
  brokers = ${lagom.broker.defaults.kafka.brokers}

  client {
    default {
      # Exponential backoff for failures
      failure-exponential-backoff {
        # minimum (initial) duration until processor is started again
        # after failure
        min = 3s

        # the exponential back-off is capped to this duration
        max = 30s

        # additional random delay is based on this factor
        random-factor = 0.2
      }
    }

    # configuration used by the Lagom Kafka producer
    producer = ${lagom.broker.kafka.client.default}
    producer.role = ""

    # configuration used by the Lagom Kafka consumer
    consumer {
      failure-exponential-backoff = ${lagom.broker.kafka.client.default.failure-exponential-backoff}

      # The number of offsets that will be buffered to allow the consumer flow to
      # do its own buffering. This should be set to a number that is at least as
      # large as the maximum amount of buffering that the consumer flow will do,
      # if the consumer buffer buffers more than this, the offset buffer will
      # backpressure and cause the stream to stop.
      offset-buffer = 100

      # Number of messages batched together by the consumer before the related messages'
      # offsets are committed to Kafka.
      # By increasing the batching-size you are trading speed with the risk of having
      # to re-process a larger number of messages if a failure occurs.
      # The value provided must be strictly greater than zero.
      batching-size = 20

      # Interval of time waited by the consumer before the currently batched messages'
      # offsets are committed to Kafka.
      # This parameter is useful to ensure that messages' offsets are always committed
      # within a fixed amount of time.
      # The value provided must be strictly greater than zero.
      batching-interval = 5 seconds
    }
  }
}

#Enable circuit breaker metrics
lagom.spi.circuit-breaker-metrics-class = "cinnamon.lagom.CircuitBreakerInstrumentation"

akka.discovery.method = akka.discovery.config

# Ready health check returns 200 when cluster membership is in the following states.
# Intended to be used to indicate this node is ready for user traffic so Up/WeaklyUp
# Valid values: "Joining", "WeaklyUp", "Up", "Leaving", "Exiting", "Down", "Removed"
akka.management.cluster.http.healthcheck.ready-states = ["Up"]

#In case of unreachable nodes or network partition, Split Brain Resolver will
#apply this strategy to repair the Akka cluster.
#akka.cluster.downing-provider-class = "com.lightbend.akka.sbr.SplitBrainResolverProvider"
#akka.cluster.split-brain-resolver.active-strategy=keep-majority

# Necessary to ensure Lagom successfully exits the JVM on shutdown.
lagom.cluster.exit-jvm-when-system-terminated = on
